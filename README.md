# DynamicGPT
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17719398.svg)](https://doi.org/10.5281/zenodo.17719398)

A minimalistic conversational model based on the **"AI = Dynamic Classification"** theory (Wang, 2025).  
It simulates language generation through **context-driven rule matching**, without any deep learning frameworks or pre-trained models.

## ðŸ§  Core Idea

- **Each generation step = a context-aware classification over the vocabulary**
- The model maintains a set of "dynamically constructed semantic categories" as rules
- Supports limited multi-turn dialogue history with basic autoregressive generation

## ðŸš€ Quick Start

```bash
python gpt.py
```

You'll see the following demo output:

```
ðŸ¤– DynamicGPT â€” Based on 'AI = Dynamic Classification' Theory

ðŸ‘¤ User: hello
ðŸ¤– AI:   hi how are you later ok ok ok ok ok ok ok ok ok ok ?

ðŸ‘¤ User: what is ai
ðŸ¤– AI:   cool !

ðŸ‘¤ User: can you do math
ðŸ¤– AI:   2 + 4 = 6 !

ðŸ‘¤ User: why is sky blue
ðŸ¤– AI:   because light scatters !

ðŸ‘¤ User: tell me about cats
ðŸ¤– AI:   they are nice !

ðŸ‘¤ User: bye
ðŸ¤– AI:   see you later ok ok ok ok ok ok ok ok ok ok ok ok
```

> âœ… Fixed issue: Removed repetitive `"ok"` tokens by improving rule consistency (e.g., `"cats"` now correctly leads to `"they"` instead of directly to `"are"`).

## ðŸ” How It Works

1. **Input Processing**: Concatenates user prompt and conversation history into a token list  
2. **Classification Decision**: `classify_next_token(context_tokens)` matches the longest context pattern:
   - Priority: 3-gram â†’ 2-gram â†’ 1-gram
   - Falls back to `"ok"` if no rule matches
3. **Autoregressive Generation**: Predicts one token at a time, appends it to context, and stops at `!`, `?`, `.` or after 15 tokens
4. **History Management**: Keeps only the most recent 20 tokens for efficiency

## ðŸ“œ Built-in Dialogue Flows

| User Input | AI Response |
|-----------|-------------|
| `hello` | `hi how are you ?` |
| `what is ai` | `cool !` |
| `can you do math` | `2 + 4 = 6 !` |
| `why is sky blue` | `because light scatters !` |
| `tell me about cats` | `they are nice !` |
| `bye` | `see you later` |

> ðŸ’¡ Note: Rules are matched in order. For example, `("you",)` appears twice; the later definition (`"later"`) takes precedence due to sequential lookup.

## âš™ï¸ Customization

Extend the model easily:
1. Add new n-gram rules to the `rules` dictionary in `gpt.py`
2. Keys must be tuples: e.g., `("how", "old", "are")`
3. Values must be single-token strings

Example:
```python
("who", "are"): "you",
("you",): "a toy model !"
```

## ðŸ“Œ Notes

- This project is for **educational/conceptual demonstration only** â€” it has **no real NLP capability**
- All logic is hardcoded in `gpt.py` with zero external dependencies
- Inspired by the cognitive science perspective that â€œlanguage is classificationâ€

---

> ðŸŽ¯ *"True intelligence may not be about predicting the next word, but dynamically constructing meaningful categories from infinite possibilities."* â€” Wang, 2025
